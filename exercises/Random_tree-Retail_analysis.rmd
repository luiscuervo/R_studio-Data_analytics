---
title: "Predicting Pregnant Customers at RetailMart"
author: "Javier Allende, Aliex Casta√±o, Luis Cuervo"
date: "Based on J.W. Foreman's book"
output: 
  html_document:
    number_section: yes
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart) #execute the rpart if I do not use kintmny
library(rpart.plot)
library(caret)
```

<h2>Source</h2>
This case study is based on the business case and dataset described in J.W.Foreman's book: "DATA SMART. Using Data Science to Transform Information into Insight", Wiley, pp. 206-209, 2014.

<h2>Introduction</h2>
Pretend you are a marketing manager at RetailMart's corporate headquarters in charge of infant merchandise. Your job i s to help sell more diapers, formula, onesies, cribs, strollers, pacifiers, etc. to new parents, but you have a problem.

You know from focus groups that new parents get into habits with baby products. They find diaper brands they like early on and stores that have the best prices on their brands.
They find the pacifier that works with their baby, and they know where to go to get the
cheap two-pack.
You want RetailMart to be the first store these new parents by diapers at.
You want to maximize RetailMart's chances of being a parent's go-to for baby purchases.

But to do that, you need to market to these parents before they buy their first package
of diapers somewhere else. 
You need market to the parents before the baby shows up. That way, when the baby arrives,
The parents have already received and possibly already used that coupon they got in the mail
for diapers and ointment.

You need a predictive model to help identify potential pregnant customers for targeted
direct marketing.

<h2> Dataset</h2>
You have a secret weapon at your disposal for building this model: customer account data.
You don't have this data for every customer; no, you're up the creek for the guy who
lives in the woods and only pays cash. But for those who use a store credit card or
have an online account tied to their major credit card, you can tie purchases not 
necessarily to an individual but at least to a household.

However, you can't just feed an entire purchase history, unstructured, into an AI model and
expect things to happen. You have to be smart about pulling relevant predictors
out of the dataset.
So the question you should ask yourself is which past purchases are predictive
for or against a household being pregnant?

The first purchase that comes to mind is a pregnancy test.
If a customer buys a pregnancy test, they're more likely to be pregnant than the average
customer. These predictors are often called model *features* or *independent variables*, 
while the thing we're trying to predict *Pregnant (yes/no)?* would be the
*dependent variable* in the sense that its value is dependent on the independent variable
data we're pushing into the model.

What purchase history should RetailMart consider?

Here's a list of example features that could be generated from a customer's purchase
records and associated account information:

<ul>
<li> Account holder is Male/Female/Unknown by matching surname to census data.
<li> Account holder address is a home, apartament, or PO box.
<li> Recently purchased a pregnancy test.
<li> Recently purchased birth control.
<li> Recently purchased feminine hygiene products.
<li> Recently purchased folic acid supplements.
<li> Recently purchased prenatal vitamins.
<li> Recently purchased prenatal yoga DVD.
<li> Recently purchased body pillow.
<li> Recently purchased ginger ale.
<li> Recently purchased Sea-Bands.
<li> Bought cigarettes regularly until recently, then stopped.
<li> Recently purchased cigarettes.
<li> Recently purchased smoking cessation products (gum, patch, etc).
<li> Bought wine regularly until recently, then stopped.
<li> Recently purchased wine.
<li> Recently purchased maternity clothing.
</ul>

None of these predictors are perfect. Customers don't buy everything at RetailMart;
a customer may choose to buy their pregnancy test at the local drug store instead of
RetailMart or their prenatal supplements might be prescription.
Even if the customer did buy everything at RetailMart, pregnant *households* can still
have a smoker or a drinker. Maternity clothing is often worn by non-pregnant folks,
especially when the Empire waist is in style. Ginger ale may help nausea, but it's also
great with bourbon. You get the picture.

None of these predictors are going to cut it, but the hope is that with their powers
combined Captain-Planet-style, the model will be able to classify customers reasonably
well.

<h2>Assembling the Training Data</h2>
Six percent of RetailMart's customer households are pregnant at any given time according
to surveys the company has conducted. You need to grab some examples of this group
from the RetailMart database and assemble your modeling features on their purchase history
before they gave birth. Likewise, you need to assemble these features for a sample
of customers who aren't pregnant. Then, you can use these known examples to train
your model.

But how should you go about identifying past pregnant households in the data? Surveying
customers to build a training set is always an option. You're just building a prototype,
so perhaps approximating households who just had a baby by looking at buying habits is 
good enough. For customers who suddenly began buying newborn diapers, you can
reasonably assume the customer's household has a new baby.
So by looking at the purchase history for the customer before the diaper-buying event,
you can assemble the features listed previously for a pregnant household.
Imagine you pull 500 examples of pregnant households and assemble their feature data
from the RetailMart database.

As for non-pregnant customers, you can assemble purchase history from a random selection
of custoers in RetailMart's database that don't meet the "ongoing diaper purchasing" criteria.
Sure, one or two pregnant people might slip into the not-pregnant category.
Imagine you grab another 500 examples of these non-pregnant customers.

#Upload the Dataset
Upload the dataset "RetailMart.csv" and look at the structure of the dataset. Count how many cases of pregnant women and not-pregnant women there are.

```{r, eval=TRUE, echo=TRUE}
ds<-read.csv("RetailMart.csv",sep=";")
ds$X<-NULL
head(ds)

positives <- length(ds$PREGNANT[ds$PREGNANT == 1])
negatives <- length(ds$PREGNANT[ds$PREGNANT == 0])
cat('pregnant cases:', positives, 'not-pregnant cases:', negatives)

#your code goes here
```

#Training a Simple Model

##Train a Tree
Using library *rpart*, train a simple decision tree, where *pregnant* is the dependent variable and 
variables *Folic.Acid* and *Prenatal.Vitamins* are independent variables that feed the model. 

To solve the case, you'll need libraries **rpart**, **rpart.plot** and **caret**. If you don't have them, install them into RStudio and upload them using instruction **library**.


```{r}
ds$PREGNANT <- as.factor(ds$PREGNANT)
tree <- rpart(PREGNANT ~ Folic.Acid + Prenatal.Vitamins, ds)
tree
```

##Tree Structure
Draw and interpret the tree.

Explain in your own words the insights extracted from this tree.


```{r}
rpart.plot(tree)
```

##Classification Accuracy
Compute the **accuracy** of the model. Do it with your own instructions. You can use *predict*.


```{r}
random.ds <- predict(tree,ds,type = "class")
compare <- data.frame(predicted=random.ds, trueclass=ds$PREGNANT)
error <- nrow(compare[compare$predicted!=compare$trueclass,])
ok <- nrow(compare[compare$predicted==compare$trueclass,])
n <- nrow(compare)

accuracy <- ok / n
accuracy

```

##Tree Traversal
By traversing the tree, predict the class of these cases:

<ul>
<li> Folic.Acid = 0; Prenatal.Vitamins = 0
<li> Folic.Acid = 0; Prenatal.Vitamins = 1
<li> Folic.Acid = 1; Prenatal.Vitamins = 0
<li> Folic.Acid = 1; Prenatal.Vitamins = 1
</ul>



##Predict
Use function *predict* with the four previous cases. Check whether you agree with the results you provided in the previous section.


```{r}
predict(tree, data.frame(Folic.Acid = 0, Prenatal.Vitamins = 0), type = "class")
predict(tree, data.frame(Folic.Acid = 0, Prenatal.Vitamins = 1), type = "class")
predict(tree, data.frame(Folic.Acid = 1, Prenatal.Vitamins = 0), type = "class")
predict(tree, data.frame(Folic.Acid = 1, Prenatal.Vitamins = 1), type = "class")

# We can see that the algorithm labels as pregnant (1 1) if the they bought either prenatal viatmins of Folic acid, which makes sense. Let's look now at the predicted probablility:

predict(tree, data.frame(Folic.Acid = 0, Prenatal.Vitamins = 0))
predict(tree, data.frame(Folic.Acid = 0, Prenatal.Vitamins = 1))
predict(tree, data.frame(Folic.Acid = 1, Prenatal.Vitamins = 0))
predict(tree, data.frame(Folic.Acid = 1, Prenatal.Vitamins = 1))

# It is less certain about non pregnant cases
```



##Confusion Matrix
Compute the *confusion matrix* for the dataset. Based on the output
of the confusion matrix, compute true positive rate and true negative rate.  Interpret these values.

Based on these values, is it easier to predict *pregnancy* or *non-pregnancy*?



```{r}
confusionMatrix(random.ds, ds$PREGNANT)

```

##Sensitivity & Specificity
Based on the output of the confusion matrix, identify the values of sensitivity and specificity.
Define their meaning and calculate these values with your own instructions, by using 
*predict*.



```{r}
# SENSITIVITY: True positives / Total positives. Number of positive samples predicted as positives over the total number of samples predicted as positives.
# SPECIFICITY: True negatives / total negatives. Number of negatuve samples predicted as negatives over the total number of samples predicted as positives.


prediction_of_positives <- predict(tree, data.frame(Folic.Acid = ds$Folic.Acid[ds$PREGNANT == 1], Prenatal.Vitamins = ds$Prenatal.Vitamins[ds$PREGNANT == 1]), type = "class")

prediction_of_negatives <- predict(tree, data.frame(Folic.Acid = ds$Folic.Acid[ds$PREGNANT == 0], Prenatal.Vitamins = ds$Prenatal.Vitamins[ds$PREGNANT == 0]), type = "class")

sensit = length(prediction_of_positives[prediction_of_positives == 1]) / length(random.ds[random.ds==1])
sensit

specific = length(prediction_of_negatives[prediction_of_positives == 0])/ length(random.ds[random.ds])
specific
# Alternatively
sensit = length(compare$trueclass[compare$predicted==1 & compare$trueclass==1]) /  length(compare$predicted[compare$predicted==1])
sensit

specific = length(compare$trueclass[compare$predicted==0 & compare$trueclass==0]) / length(compare$predicted[compare$predicted==0])
specific

#Does not seem to be working  ¬Ø\_(„ÉÑ)_/¬Ø

```



#Train a more Complex Model

##Train a Tree

Using library *rpart*, train a decision tree where *pregnant* is the dependent variable and 
all the other variables are independent variables that feed the model. 


```{r}
ds$PREGNANT <- as.factor(ds$PREGNANT)
tree2 <- rpart(PREGNANT~ ., ds)
tree2
```

##Tree Structure
Draw and interpret the tree.

```{r}
rpart.plot(tree2) 
```

##Classification Accuracy
Compute the classification accuracy of the tree. Has the previous classification accuracy being improved? Why?

```{r}
random.ds.2 <- predict(tree2,ds,type = "class")
compare.2 <- data.frame(predicted=random.ds.2, trueclass=ds$PREGNANT)
error.2 <- nrow(compare.2[compare.2$predicted!=compare.2$trueclass,])
ok.2 <- nrow(compare.2[compare.2$predicted==compare.2$trueclass,])
n.2 <- nrow(compare.2)

accuracy.2 <- ok.2 / n.2
accuracy.2

# It has improved significantly (from 0.676 to 0.804) because the algorithm has more data to work with in each sample. Note that folic acid and prenatal vitamins are on top of the tree, which makes them more relevant than the rest of the data
```

##Tree Traversal
Convert the left most branch of the tree and the right most branch of the tree into two independent classification rules.
How many different classification rules can be extracted from the tree?

##Confusion Matrix
Compute the confusion matrix of the tree.

```{r}
confusionMatrix (random.ds.2, ds$PREGNANT)
```

##Sensitivity & Specificity
Identify the value of sensitivity and specificity. In what sense have the results improved?

Sensitivity : 0.8500         
Specificity : 0.7580 
It has increased drastically in specificity and decreased a little in sensitivity, meaning it can identify much better non-pregnant women. This is a great improvement because the preivous algorithm tended to predict all of the data as pregnant, which its high sensitivity irrelevant.


#Train-Test Models
To avoid optimistic estimates of the accuracy of the model, we'll train the tree with a
training dataset, which will contain 75% of the examples of the original dataset. 
We'll keep the remaining 25% examples in the test dataset.



##Create the Partition
Create the two partitions: *train.ds* and *test.ds*. You can use function *createDataPartition* from library *caret*.

```{r}
ds$PREGNANT <- as.factor(ds$PREGNANT)
ds$PREGNANT
train <- createDataPartition (y=ds$PREGNANT, p=0.75,list=FALSE)
train.ds <- ds[train,]
test.ds <- ds[-train,]
```

##Train the Tree
Train the tree with dataset *train.ds* and all the variables. 

```{r}
tree3 <- rpart(PREGNANT~ ., train.ds)
tree3
rpart.plot(tree3)
```

##Training Classification Accuracy
Compute the classification accuracy and the confusion matrix over the training data. You can use functions *predict* and *confusionMatrix*.

```{r}
random.ds.3 <- predict(tree3,train.ds,type = "class")
compare.3 <- data.frame(predicted=random.ds.3, trueclass=train.ds$PREGNANT)
error.3 <- nrow(compare.3[compare.3$predicted!=compare.3$trueclass,])
ok.3 <- nrow(compare.3[compare.3$predicted==compare.3$trueclass,])
n.3 <- nrow(compare.3)

accuracy.3 <- ok.3 / n.3
accuracy.3
```

##Test Classification Accuracy
Compute the classification accuracy and the confusion matrix over the test data. You can use functions *predict* and *confusionMatrix*. 

```{r}
test.3 <- predict(tree3,test.ds,type = "class")
confusionMatrix (test.3, test.ds$PREGNANT)


random.ds.4 <- predict(tree3,test.ds,type = "class")
compare.4 <- data.frame(predicted=random.ds.4, trueclass=test.ds$PREGNANT)
error.4 <- nrow(compare.4[compare.4$predicted!=compare.4$trueclass,])
ok.4 <- nrow(compare.4[compare.4$predicted==compare.4$trueclass,])
n.4 <- nrow(compare.4)

accuracy.4 <- ok.4 / n.4
accuracy.4

```

Also, compute classification accuracy over the test daaset without using *confusionMatrix*. 

Well, we didn't do it with confussion matrix either way

##Reflection
Compare the accuracy of the model computed over the traning dataset and the test dataset. Explain the differences, if any. Then, compare these results with the output of the previous model, when you trained the tree with the full dataset. Explain why it is important to perform train-test partitions to evaluate the algorithm.


The training accuracy is higher than the testing accuracy. That makes sense, because the algorithm has already learned from the training data, so it is more likely to predict it right. On the other hand, it has never seen the testing data so the obtained accuracy is more reliable.

#Insights
At this point, we have seen two applications of data analysis algorithms:

<ul>
<li> Prediction of sales, using regression analysis.
<li> Prediction of pregnancy, using classification analysis, and particulary, decision trees.
</ul>

Explain the diffeences and similarities between the two approaches, in terms of:


<ul>
<li> Structure of the dataset

The data set for regression is composed of continuous data, whereas in classification, it is composed of discrete data.

<li> Types of variables.

In regression analysis, output variables are numerical, whereas in classification analysis they are binary or categorical.

<li> Types of algorithms.

For regression analysis we have the following algorithms:
-Linear regression and other mathematical models
-Decision trees adapted for regression
-Nearest neighbors
-Neural networks

For classification analysis we have:
-Decision trees
-Nearest neighbors
-Neural networks

<li> Description of relationships between variables.

Regression analysis draws a function or a model that relates a dependent variable with one or more independent variables.
Classification analysis describes and distinguishes data classes or concepts in order to build a model to predict the class of objects whose class label is unknown. 

<li> Explanatory capabilities of these models.

Regression tries to find the best fit line to predict the output more accurately, whereas classification tries to find the decision boundary to divide the dataset into different classes.

<li> Goodness of fit of the two algorithms.

We have two methods to determine the goodness of fit for regression analysis:
-R squared
-Error

In classification analysis we have:
-Classification accuracy
-Confusion matrix

<li> Train-test procedures.
</ul>



#K-NN
```{r}
knn.fit <- train(PREGNANT~ ., data=train.ds, method="knn", preProcess=c("center", "scale"))
knn.predict <- predict(knn.fit, newdata=test.ds)

confusionMatrix (knn.predict, test.ds$PREGNANT)

# We achieved similar
```
